{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzwGit5nE4x_"
      },
      "source": [
        "## Import the libraries\n",
        "Importing necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhZP3exS_4NB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWIM6gplHqfx"
      },
      "source": [
        "## Import the dataset\n",
        "\n",
        "Import the training file, load it into your workspace and put the sentences and labels into lists. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrdK34A0WNwU",
        "outputId": "7278bdee-0f8f-4057-cdd1-188e8832041e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "jq is already the newest version (1.6-1ubuntu0.20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n",
            "rm: cannot remove 'data.jsonl': No such file or directory\n",
            "rm: cannot remove 'data.jsonl.gz': No such file or directory\n",
            "--2023-05-31 05:37:13--  https://huggingface.co/datasets/dair-ai/emotion/resolve/main/data/data.jsonl.gz\n",
            "Resolving huggingface.co (huggingface.co)... 99.84.191.118, 99.84.191.107, 99.84.191.66, ...\n",
            "Connecting to huggingface.co (huggingface.co)|99.84.191.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/emotion/8944e6b35cb42294769ac30cf17bd006231545b2eeecfa59324246e192564d1f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27data.jsonl.gz%3B+filename%3D%22data.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1685770633&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL2RhdGFzZXRzL2Vtb3Rpb24vODk0NGU2YjM1Y2I0MjI5NDc2OWFjMzBjZjE3YmQwMDYyMzE1NDViMmVlZWNmYTU5MzI0MjQ2ZTE5MjU2NGQxZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODU3NzA2MzN9fX1dfQ__&Signature=NHuE0fCEArnC7hFUJKN0PbWXM5t3dKEzTM82MWFJWccYTXnmgLBo2lFmk8dbOQGHPYc-iOLlJeJSUliz1lL92p%7ESob8NThSPY1BTlY3wGHxR81cr-s1AdGo3rOoYGyufHAF4MGKsTp94u75tEXN2GHA41XV2PMBgg%7EPMtiNT1f2fX9BYtFzE53%7EGG-Q7xDiGLjw3InvSGft8rwqK1JOh03xMjHZzFhfLxciHAQ7R53Xnx2K0%7EwaNEMhUru7GrKCpUyHArTgktxDjQgG7Z-ehlPbmAMUipzss9ccAnT2NiqU0X5tswuVePz-jHqdBk5nHveqNFRrsZfz3g30r7RwGnA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-05-31 05:37:13--  https://cdn-lfs.huggingface.co/datasets/emotion/8944e6b35cb42294769ac30cf17bd006231545b2eeecfa59324246e192564d1f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27data.jsonl.gz%3B+filename%3D%22data.jsonl.gz%22%3B&response-content-type=application%2Fgzip&Expires=1685770633&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL2RhdGFzZXRzL2Vtb3Rpb24vODk0NGU2YjM1Y2I0MjI5NDc2OWFjMzBjZjE3YmQwMDYyMzE1NDViMmVlZWNmYTU5MzI0MjQ2ZTE5MjU2NGQxZj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODU3NzA2MzN9fX1dfQ__&Signature=NHuE0fCEArnC7hFUJKN0PbWXM5t3dKEzTM82MWFJWccYTXnmgLBo2lFmk8dbOQGHPYc-iOLlJeJSUliz1lL92p%7ESob8NThSPY1BTlY3wGHxR81cr-s1AdGo3rOoYGyufHAF4MGKsTp94u75tEXN2GHA41XV2PMBgg%7EPMtiNT1f2fX9BYtFzE53%7EGG-Q7xDiGLjw3InvSGft8rwqK1JOh03xMjHZzFhfLxciHAQ7R53Xnx2K0%7EwaNEMhUru7GrKCpUyHArTgktxDjQgG7Z-ehlPbmAMUipzss9ccAnT2NiqU0X5tswuVePz-jHqdBk5nHveqNFRrsZfz3g30r7RwGnA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.64.36, 108.138.64.121, 108.138.64.111, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.64.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15388281 (15M) [application/gzip]\n",
            "Saving to: ‘data.jsonl.gz’\n",
            "\n",
            "data.jsonl.gz       100%[===================>]  14.67M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-31 05:37:13 (312 MB/s) - ‘data.jsonl.gz’ saved [15388281/15388281]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install jq\n",
        "!rm data.json data.jsonl data.jsonl.gz\n",
        "!wget https://huggingface.co/datasets/dair-ai/emotion/resolve/main/data/data.jsonl.gz\n",
        "!gunzip data.jsonl.gz\n",
        "!jq --slurp . < data.jsonl > data.json\n",
        "!rm data.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX_NjwF7JdZw"
      },
      "outputs": [],
      "source": [
        "# Load the JSON file\n",
        "with open(\"data.json\", 'r') as f:\n",
        "  datastore = json.load(f)\n",
        "\n",
        "  # Initialize the lists\n",
        "  sentences = []\n",
        "  labels = []\n",
        "\n",
        "  # Collect sentences and labels into the lists\n",
        "  for item in datastore:\n",
        "      sentences.append(item['text'])\n",
        "      labels.append(item['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jiha1x0-_nIh"
      },
      "outputs": [],
      "source": [
        "vocab_size = 100000  # Maximum vocab size.\n",
        "max_len = 32  # Sequence length to pad the outputs to.\n",
        "embedding_dim = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wiaI2lbBJ5-7"
      },
      "outputs": [],
      "source": [
        "training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meQNzcdVBgau"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WphManHUXkW8"
      },
      "source": [
        "Establish the text vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVkGm92k_s0c"
      },
      "outputs": [],
      "source": [
        "vectorize_layer = tf.keras.layers.TextVectorization(\n",
        " max_tokens=vocab_size,\n",
        " standardize='lower',\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)\n",
        "\n",
        "vectorize_layer.adapt(training_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AI4XBj9XoMV"
      },
      "source": [
        "Remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4guvEgn5WCDh",
        "outputId": "286d3a5b-4c6b-4e74-f232-b6f580fd6988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "vocabs = vectorize_layer.get_vocabulary()\n",
        "\n",
        "for stop_word in stop_words:\n",
        "  if stop_word in vectorize_layer.get_vocabulary():\n",
        "    vocabs.remove(stop_word)\n",
        "\n",
        "vectorize_layer.set_vocabulary(vocabs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH9_9EtwXiJ1"
      },
      "source": [
        "Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i5Nzb8Y_wmh",
        "outputId": "09a670f4-6936-4809-84ee-0b2609886921"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVec  (None, 32)               0         \n",
            " torization)                                                     \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 32, 32)            3200000   \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 32, 32)           0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 32, 64)           16640     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 30, 32)            6176      \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 32)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                528       \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 6)                 102       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,223,446\n",
            "Trainable params: 3,223,446\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create the model that uses the vectorize text layer\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "    vectorize_layer,\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n",
        "    tf.keras.layers.SpatialDropout1D(0.5),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
        "    tf.keras.layers.GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(16, kernel_regularizer=tf.keras.regularizers.L2(0.001), activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "\n",
        "# Summary of Model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7XgXwtqC_cZ"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65dxqcRTDIXT",
        "outputId": "6acb6f2e-25e8-4125-87fb-dc961fbc25fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "10421/10421 - 863s - loss: 0.2902 - accuracy: 0.8819 - val_loss: 0.1482 - val_accuracy: 0.9238 - 863s/epoch - 83ms/step\n",
            "Epoch 2/30\n",
            "10421/10421 - 852s - loss: 0.1518 - accuracy: 0.9239 - val_loss: 0.1382 - val_accuracy: 0.9235 - 852s/epoch - 82ms/step\n",
            "Epoch 3/30\n",
            "10421/10421 - 847s - loss: 0.1388 - accuracy: 0.9259 - val_loss: 0.1327 - val_accuracy: 0.9233 - 847s/epoch - 81ms/step\n",
            "Epoch 4/30\n",
            "10421/10421 - 848s - loss: 0.1325 - accuracy: 0.9266 - val_loss: 0.1302 - val_accuracy: 0.9259 - 848s/epoch - 81ms/step\n",
            "Epoch 5/30\n",
            "10421/10421 - 881s - loss: 0.1285 - accuracy: 0.9275 - val_loss: 0.1293 - val_accuracy: 0.9256 - 881s/epoch - 85ms/step\n",
            "Epoch 6/30\n",
            "10421/10421 - 855s - loss: 0.1253 - accuracy: 0.9289 - val_loss: 0.1297 - val_accuracy: 0.9254 - 855s/epoch - 82ms/step\n",
            "Epoch 7/30\n",
            "10421/10421 - 864s - loss: 0.1224 - accuracy: 0.9299 - val_loss: 0.1310 - val_accuracy: 0.9251 - 864s/epoch - 83ms/step\n",
            "Epoch 8/30\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(training_sentences, training_labels, epochs=num_epochs, validation_data=(testing_sentences, testing_labels), callbacks=[early_stopping], verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cswUWV5TRadJ"
      },
      "source": [
        "## Testing preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3yddvh0_05H"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "emotion_labels = ['Sadness', 'Joy', 'Love', 'Anger', 'Fear', 'Surprise']  # Emotion labels based on your dataset\n",
        "\n",
        "user_inputs = testing_sentences[:10]\n",
        "result_detail = 0\n",
        "\n",
        "# Predict emotions for user inputs\n",
        "predictions = model.predict(user_inputs)\n",
        "\n",
        "# Print the predicted emotions and their percentages for each user input\n",
        "for i, text in enumerate(user_inputs):\n",
        "    print(f\"Sentence: {text}\")\n",
        "    if i < len(predictions):\n",
        "      if result_detail == 1:\n",
        "        prediction = predictions[i]\n",
        "        predicted_percentages = [percentage * 100 for percentage in prediction]\n",
        "        emotion_percentages = {\n",
        "            emotion: percentage\n",
        "            for emotion, percentage in zip(emotion_labels, predicted_percentages)\n",
        "        }\n",
        "        print(\"Emotion Percentages:\")\n",
        "        for emotion, percentage in emotion_percentages.items():\n",
        "            print(f\"{emotion}: {percentage:.2f}%\")\n",
        "      if result_detail == 0:\n",
        "        print(f\"Predicted Emotion: {[emotion_labels[prediction.argmax()] for prediction in predictions][i]}\")\n",
        "    else:\n",
        "        print(\"Unable to predict emotion for this sentence.\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlJf6hi4F80C"
      },
      "source": [
        "## Convert into TensorFlow Lite\n",
        "Converting model into TFLite then export it "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d5PoxQfFVif"
      },
      "outputs": [],
      "source": [
        "# Convert the TensorFlow model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.experimental_new_converter=True\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,\n",
        "tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TensorFlow Lite model to a file\n",
        "tflite_model_path = \"./tflite_model_v3.2.tflite\"\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIV3-k8uOlFM"
      },
      "source": [
        "Convert the labels into .json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxUcskgMOmb3"
      },
      "outputs": [],
      "source": [
        "# Export the label dict\n",
        "with open( './labels.json' , 'w' ) as file:\n",
        "  json.dump( emotion_labels , file )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model from local to drive"
      ],
      "metadata": {
        "id": "DTXirQyITmKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Course/Bangkit/Capstone/TFLite Models/tflite_model_v3.2.tflite', 'w') as createdModel:\n",
        "  createdModel.write('Model saved to drive')"
      ],
      "metadata": {
        "id": "_-dDEqSwRHaP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}